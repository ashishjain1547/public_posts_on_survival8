<head>
    <script crossorigin="anonymous" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" src="https://code.jquery.com/jquery-3.2.1.min.js"></script>

    <!-- Google AdSense Using Machine Learning Code -->
    <script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
        (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-3071098372371409",
            enable_page_level_ads: true
        });
    </script>

    <script>
        $(document).ready(function () {
            $.ajax({
                url: "https://raw.githubusercontent.com/ashishjain1547/pubLessonsInTechnology/main/links_to_tech_clubs.json",
                success: function (result) {
                    let grouplink = JSON.parse(result)['Beta Tech Club'];
                    $("#customWhatsAppGroupLinkWrapper").html(
                        `
                        <h2 class="custom_link_h2"><a href="${grouplink}" target="_blank"> 
                            <span>Join us on:</span>
                            <span class="customLink"><i class="fa fa-whatsapp"></i> Whatsapp </span>
                            </a>
                        </h2>
                        `
                    );
                }
            });
        });
    </script>

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"></link>

    <style>
        pre {
            white-space: pre-wrap;
            white-space: -moz-pre-wrap;
            white-space: -pre-wrap;
            white-space: -o-pre-wrap;
            word-wrap: break-word;
        }

        .customLink {
            background-color: #4CAF50;
            border: none;
            color: white !important;
            padding: 8px 13px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
        }

        .customLink:hover {
            text-decoration: none;
        }

        div.code-block-decoration.footer {
            display: none;
        }

        button.export-sheets-button-wrapper {
            display: none;
        }
    </style>

    <style>
        .custom_link_h2 a {
            color: black;
            text-decoration: none;
            text-align: center;
        }

        .custom_link_h2 a:hover {
            color: black;
        }

        .custom_link_h2 a:active {
            color: black;
        }

        .custom_link_h2 span {
            translate: 0px -5px;
            display: inline-block;
        }

        .custom_link_h2 img {
            width: 100px;
            padding: 0px;
            border: none;
            box-shadow: none;
        }
    </style>
    <style>
        .customul {
            list-style: none;
        }

        [aria-hidden='true'] {
            display: none;
        }

        .custom_iframe {
            width: 100%;
            height: 305px;
        }

        i.ib {
            color: blue;
        }

        i.ig {
            color: green;
        }

        .customTable td {
            padding: 2px;
        }
</style>

</head>

<div id="customWhatsAppGroupLinkWrapper"></div>

<a class="customLink" href="https://survival8.blogspot.com/p/personal-posts-index.html#customAINews" target="_blank">See All Articles</a>
<br />
<br />
<div class="separator" style="clear: both;"><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2nj_t0PEbRHxj-jrmAQ6IEd1BbZTOUtuvYCJjL7IxmBMcenvsHkB0J2qYXvKykqRhlmqruGCugmmC7uAhCrfeJEg13scN2tdWO-q_oZABEPxNpwqhKik5thNp1Ojm7ShFu9WoTfbo-rGQr1n4eu5tplRC6PXblkiQ-3kWdzIX7y7YSNw4ozt3FkJhkWXo/s500/Gemini_Generated_Image_rhkcl3rhkcl3rhkc.png" style="display: block; padding: 1em 0; text-align: center; "><img alt="" border="0" width="600" data-original-height="500" data-original-width="500" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2nj_t0PEbRHxj-jrmAQ6IEd1BbZTOUtuvYCJjL7IxmBMcenvsHkB0J2qYXvKykqRhlmqruGCugmmC7uAhCrfeJEg13scN2tdWO-q_oZABEPxNpwqhKik5thNp1Ojm7ShFu9WoTfbo-rGQr1n4eu5tplRC6PXblkiQ-3kWdzIX7y7YSNw4ozt3FkJhkWXo/s600/Gemini_Generated_Image_rhkcl3rhkcl3rhkc.png"/></a></div>

<br><br>
<iframe class="custom_iframe" src="https://www.youtube.com/embed/QGqxR2CKddk" title="China Just Dropped A Trillion Parameter Beast Crushing Top AI Models" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br><br>
<div class="markdown prose dark:prose-invert w-full break-words light markdown-new-styling">

<p data-start="290" data-end="494">The global AI race just hit another gear. In a single week, China unleashed not one but <strong data-start="378" data-end="414">two trillion-parameter AI models</strong>, shaking up the leaderboard and putting pressure on American labs to respond.</p>
<h2 data-start="496" data-end="551">Alibaba’s Qwen-3 Max: A Trillion-Parameter Preview</h2>
<p data-start="553" data-end="702">The biggest headline comes from Alibaba’s Qwen team, which unveiled <strong data-start="621" data-end="643">Qwen-3 Max Preview</strong> — a model weighing in at over <strong data-start="674" data-end="699">1 trillion parameters</strong>.</p>
<p data-start="704" data-end="913">For context, many have speculated that OpenAI’s GPT-4o and its successors sit in a similar range, but most labs lately have leaned toward smaller, more efficient models. Qwen going <em data-start="885" data-end="893">bigger</em> bucks that trend.</p>
<p data-start="915" data-end="1106">Benchmarks show why: on tests like <strong data-start="950" data-end="1015">SuperGQA, LiveCodeBench V6, Arena Hard V2, and LiveBench 2024</strong>, Qwen-3 Max outperformed rivals including <strong data-start="1058" data-end="1103">Claude Opus 4, Kimi K2, and DeepSeek v3.1</strong>.</p>
<p data-start="1108" data-end="1189">That’s no small feat — these are some of the toughest models to beat right now.</p>
<h3 data-start="1191" data-end="1221">Availability and Pricing</h3>
<p data-start="1223" data-end="1252">Qwen-3 Max is already live:</p>
<ul data-start="1253" data-end="1487">
<li data-start="1253" data-end="1315">
<p data-start="1255" data-end="1315">Available via <strong data-start="1269" data-end="1282">Qwen Chat</strong> (Alibaba’s ChatGPT competitor)</p>
</li>
<li data-start="1316" data-end="1362">
<p data-start="1318" data-end="1362">Accessible through <strong data-start="1337" data-end="1360">Alibaba Cloud’s API</strong></p>
</li>
<li data-start="1363" data-end="1487">
<p data-start="1365" data-end="1487">Integrated into <strong data-start="1381" data-end="1395">OpenRouter</strong> and <strong data-start="1400" data-end="1418">Anyscale Coder</strong> (Hugging Face’s coding tool), where it’s now the <strong data-start="1468" data-end="1485">default model</strong></p>
</li>
</ul>
<p data-start="1489" data-end="1667">But unlike some of Qwen’s earlier releases, this one <strong data-start="1542" data-end="1563">isn’t open source</strong>. Access comes via Alibaba Cloud or its partners, with <strong data-start="1618" data-end="1636">tiered pricing</strong> depending on context length:</p>
<ul data-start="1669" data-end="1843">
<li data-start="1669" data-end="1747">
<p data-start="1671" data-end="1747">Up to 32k tokens: $0.86 per million input tokens, $3.44 per million output</p>
</li>
<li data-start="1748" data-end="1794">
<p data-start="1750" data-end="1794">32k–128k tokens: $1.43 input, $5.73 output</p>
</li>
<li data-start="1795" data-end="1843">
<p data-start="1797" data-end="1843">Up to 252k tokens: $2.15 input, $8.60 output</p>
</li>
</ul>
<p data-start="1845" data-end="1912">Short prompts? Affordable. Heavy, high-context workloads? Pricey.</p>
<h3 data-start="1914" data-end="1947">Context Window and Features</h3>
<ul data-start="1949" data-end="2255">
<li data-start="1949" data-end="2094">
<p data-start="1951" data-end="1984"><strong data-start="1951" data-end="1967">Max context:</strong> 262,144 tokens</p>
<ul data-start="1987" data-end="2094">
<li data-start="1987" data-end="2017">
<p data-start="1989" data-end="2017">Input up to 258,048 tokens</p>
</li>
<li data-start="2020" data-end="2094">
<p data-start="2022" data-end="2094">Output up to 32,768 tokens (trade-off between input vs. output length)</p>
</li>
</ul>
</li>
<li data-start="2095" data-end="2171">
<p data-start="2097" data-end="2171"><strong data-start="2097" data-end="2116">Context caching</strong>: keeps long conversations alive without reprocessing</p>
</li>
<li data-start="2172" data-end="2255">
<p data-start="2174" data-end="2255"><strong data-start="2174" data-end="2187">Use cases</strong>: complex reasoning, coding, JSON/data handling, and creative work</p>
</li>
</ul>
<p data-start="2257" data-end="2477">Early testers (including <em data-start="2282" data-end="2295">VentureBeat</em>) report that it’s <strong data-start="2314" data-end="2330">blazing fast</strong> — even quicker than ChatGPT in side-by-side trials — while avoiding common “big model” pitfalls like miscounting letters or botching arithmetic.</p>
<h2 data-start="2479" data-end="2513">Moonshot AI: The Kimi Upgrade</h2>
<p data-start="2515" data-end="2662">While Qwen stole headlines, <strong data-start="2543" data-end="2558">Moonshot AI</strong>, a Beijing startup valued at <strong data-start="2588" data-end="2604">$3.3 billion</strong>, also made waves with an update to its <strong data-start="2644" data-end="2659">Kimi series</strong>.</p>
<ul data-start="2664" data-end="2922">
<li data-start="2664" data-end="2776">
<p data-start="2666" data-end="2776">The new release (internally dubbed <strong data-start="2701" data-end="2717">Kimi K2-0905</strong>) doubles the context window from 128k to <strong data-start="2759" data-end="2774">256k tokens</strong></p>
</li>
<li data-start="2777" data-end="2848">
<p data-start="2779" data-end="2848">Focuses on <strong data-start="2790" data-end="2816">improved coding skills</strong> and <strong data-start="2821" data-end="2846">reduced hallucination</strong></p>
</li>
<li data-start="2849" data-end="2922">
<p data-start="2851" data-end="2922">Keeps its creative writing strengths that made the first Kimi popular</p>
</li>
</ul>
<p data-start="2924" data-end="3178">Moonshot’s first trillion-parameter model, <strong data-start="2967" data-end="2978">Kimi K2</strong>, was open source and climbed the LM Arena leaderboard (tied for 8th overall, 4th in coding). The company remains committed to <strong data-start="3105" data-end="3136">open-sourcing future models</strong>, unlike Alibaba’s more closed approach.</p>
<p data-start="3180" data-end="3225">Founder <strong data-start="3188" data-end="3203">Yang Jullin</strong> has been outspoken:</p>
<ul data-start="3226" data-end="3510">
<li data-start="3226" data-end="3308">
<p data-start="3228" data-end="3308">Believes <strong data-start="3237" data-end="3259">millions of tokens</strong> are needed for AI to truly solve hard problems</p>
</li>
<li data-start="3309" data-end="3417">
<p data-start="3311" data-end="3417">Argues that scaling laws are alive and well, with <strong data-start="3361" data-end="3381">efficiency gains</strong> driving faster progress than ever</p>
</li>
<li data-start="3418" data-end="3510">
<p data-start="3420" data-end="3510">Revealed that <strong data-start="3434" data-end="3474">K2 is already being used to train K3</strong>, their next-generation base model</p>
</li>
</ul>
<h2 data-start="3512" data-end="3546">What It Means for the AI Race</h2>
<p data-start="3548" data-end="3689">With Alibaba and Moonshot both flexing trillion-parameter models in the same week, it’s clear that <strong data-start="3647" data-end="3686">China is serious about AI supremacy</strong>.</p>
<ul data-start="3691" data-end="4094">
<li data-start="3691" data-end="3833">
<p data-start="3693" data-end="3833"><strong data-start="3693" data-end="3708">Enterprises</strong> now have access to longer context windows and more powerful reasoning engines — but they’ll need to weigh costs and risks.</p>
</li>
<li data-start="3834" data-end="3950">
<p data-start="3836" data-end="3950"><strong data-start="3836" data-end="3850">Developers</strong> are already running into Qwen-3 Max inside tools like Anyscale Coder, often without realizing it.</p>
</li>
<li data-start="3951" data-end="4094">
<p data-start="3953" data-end="4094"><strong data-start="3953" data-end="3997">The open-source vs. closed-source divide</strong> between Qwen and Moonshot could shape the global AI ecosystem just as much as raw performance.</p>
</li>
</ul>
<p data-start="4096" data-end="4177">The bigger question: does this mark the start of China overtaking the US in AI?</p>
<p data-start="4179" data-end="4329">For now, what’s certain is that the competition just got fiercer — and trillion-parameter models are no longer the exception, but the new benchmark.</p>

</div>

<span style="display: none;">Tags: Technology,Large Language Models,Artificial Intelligence,</span>